{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributed Pandas on a cluster with Dask dataframes\n",
    "\n",
    "http://matthewrocklin.com/blog/work/2017/01/12/dask-dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas provides an intuitive, powerful and fast data analysis experienced on tabular data. However, because Pandas uses only one thread of execution and requires all data to be in memory at once, it doesn't scale well to datasets much beyond the gigabye scale. That component is missing. To fill this gap, people generally move to Spark DataFrames on HDFS or a proper relational database to resolve this scaling issue. Dask is a Python library for parallel and distributed computing that aims to fill this need for *parallelism among the PyData projects (NumPy, Pandas, Scikit-Learn, etc*). Dask dataframes combine Dask and Pandas to deliver a faithful \"big data\" version of Pandas operating in parallel over a cluster. \n",
    "\n",
    "In this exercise we will look at performance, fast shuffles and the parquet format. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connect a client to our Dask cluster. Dask cluster composed of: \n",
    "- centralised `dask-scheduler` process and several other `dask-worker` processes running on each machine in our cluster.\n",
    "- in the blogposts example, a cluster is an eight node cluster on EC@ of `m4.2xlarges` (eight cores with 30GB RAM each)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`dask.dataframe` actually looks and feels just like Pandas, although it's actually coordinating hundreds of small Pandas dataframes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cuts up our 12 CSV files on S3 into a few hundred blocks of bytes, each 64MB eahc. on each of these 64MB blocks we then call `pandas.read_csv` to create a few hundred Pandas dataframes across our cluster, one for each block of bytes. Our single Dask dataframe object, `df`, coordinates all of those Pandas dataframes. Because we're just using Pandas calls its very easy for Dask dataframes to use all of the tricks from Pandas. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributed NumPy on a Cluster with Dask Arrays \n",
    "\n",
    "http://matthewrocklin.com/blog/work/2017/01/17/dask-images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We analyze a stack of images in parallel with NumPy arrays distributed across a cluster of machines on Amazonâ€™s EC2 with Dask array. This is a model application shared among many image analysis groups ranging from satellite imagery to bio-medical applications. We go through a series of common operations:\n",
    "\n",
    "Inspect a sample of images locally with Scikit Image\n",
    "Construct a distributed Dask.array around all of our images\n",
    "Process and re-center images with Numba\n",
    "Transpose data to get a time-series for every pixel, compute FFTs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
